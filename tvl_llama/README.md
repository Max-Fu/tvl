# TVL-LLaMA
## Setup 
### LLaMA2 Setup
Please request access to the pre-trained LLaMA-2 from this [form](https://llama.meta.com/llama-downloads/). In particular, we use `llama-2-7b` as the base model. The weights here contains the trained [adapter](https://arxiv.org/abs/2309.03905), the tactile encoder, and the vision encoder for the ease of loading. 

We have the following structure for our LLaMA-2 folder:
```bash 
llama-2
├── llama-2-7b
│   ├── checklist.chk
│   ├── consolidated.00.pth
│   └── params.json
└── tokenizer.model
```

### Dataset Setup 
To setup our dataset, please refer to [README.md](../README.md). We also use CC3M, Alpaca GPT4, and LLaVA Instruct 150K as the training dataset. 
- [CC3M](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K) is used for pre-training. Please update the downloaded `csv` path in [pretrain-data-config.yaml](exps/pretrain-data-config.yaml). 
- [Alpaca GPT4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data.json) is used for finetuning. Please update the downloaded `json` path in [finetune-data-config.yaml](exps/finetune-data-config.yaml). 
- [LLaVA Instruct 150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) is used for finetuning. Please update the downloaded `json` path in [finetune-data-config.yaml](exps/finetune-data-config.yaml). 

Please also verify the correctness of the path for each of the training files in [pretrain-data-config.yaml](exps/pretrain-data-config.yaml) and [finetune-data-config.yaml](exps/finetune-data-config.yaml).

## Training
We follow the same pre-training hyperparameters as [ImageBind-LLM](https://github.com/OpenGVLab/LLaMA-Adapter/tree/main/imagebind_LLM). Please fill out `$llama_path`, `$output_dir`, `$encoder_ckpt_path`, and optionally `$log_name` if you use [wandb](https://wandb.ai). 
```bash 
CUDA_VISIBLE_DEVICES=0,1,2,3 python -u -m torch.distributed.launch --master_port=1112 --nproc_per_node=4 --use_env main_pretrain.py --data_config exps/pretrain-data-config.yaml --batch_size 8 --seed 1 --epochs 150 --split_epoch 50 --warmup_epochs 5 --blr 1.0e-4 --weight_decay 0.05 --llama_path $llama_path --output_dir $output_dir --active_modality_names vision tactile --checkpoint_path $encoder_ckpt_path --tactile_model vit_base_patch16_224 --log_name $log_name --crop_tacvis
```
- Please adjust the `--batch_size` and `--accum_iter` based on the availability and memory usage of GPUs. 
- For ViT-Tiny and ViT-Small, change `--checkpoint_path` and `--tactile_model` to `vit_small_path16_224` or `vit_tiny_path16_224` accordingly. 
- Optionally, you can only train with `vision` or `tactile` by changing the input to the flag `--active_modality_names`. 

## Finetuning
We follow the same pre-training hyperparameters as [ImageBind-LLM](https://github.com/OpenGVLab/LLaMA-Adapter/tree/main/imagebind_LLM). Please fill out `$llama_path`, `$output_dir`, `$projector_ckpt_path`, `$encoder_ckpt_path`, and optionally `$log_name` if you use [wandb](https://wandb.ai). 

```bash 
CUDA_VISIBLE_DEVICES=0,1,2,3 python -u -m torch.distributed.launch --master_port=1112 --nproc_per_node=2 --use_env main_finetune.py --data_config exps/finetune-data-config.yaml --batch_size 2 --accum_iter 4 --llama_type llama-2-7b --epochs 4 --warmup_epochs 1 --blr 10e-4 --weight_decay 0.02 --llama_path $llama_path --output_dir $output_dir  --pretrained_path $projector_ckpt_path --active_modality_names tactile vision --checkpoint_path $encoder_ckpt_path --tactile_model vit_small_patch16_224 --log_name $log_name --crop_tacvis
```

## TVL-Benchmark
### GPT-4 Evaluation
We use GPT-4 to perform evaluation. In order to run the evaluation, first obtain your [OPENAI_API_KEY](https://platform.openai.com/api-keys). Then replace `$$OPENAI_KEY`, `$ckpt_path`, `$dataset_dir`, `$llama_dir`, `$encoder_ckpt_path` accordingly:
```bash
OPENAI_API_KEY=$OPENAI_KEY python evaluate.py --has_lora --model_path $ckpt_path --gpt --active_modality_names tactile vision --tactile_model vit_small_patch16_224 --checkpoint_path $encoder_ckpt_path --eval_datasets ssvtp hct --datasets_dir $dataset_dir --llama_dir $llama_dir --crop_tacvis
```
The script will generate a json file that contains the model generated outputs and GPT4 Rating.
- Optionally, for the zero-shot single modality as input experiments, you can change `--active_modality_names` and only provide one modality as input, *i.e.* `--active_modality_names tactile`
- If you train the model with tactile background subtraction, add `--background_sub`
- If GPT-4 stops or the program crashes, provide the generated json file path using the flag `--resume_from_json` to resume evaluation. 

### Paired T-Test
Using the json file generated in GPT-4 Evaluation, we run the following script for paired t-test, with the null hypothesis being both samples are generated by GPT-4V. The generations of GPT-4V are provided in [gpt4v.json](results/gpt4v.json). 
```bash 
python t_test.py --tvl_path $json_path
```
For the provided models, the generated results and the t-test are provided in the `results` folder. For example, for TVL-LLaMA using ViT-S as the tactile encoder, we can run 
```bash 
python t_test.py --tvl_path results/tvl_llama_vits.json
```

Again, the checkpoints for TVL-LLaMA are provided below:
<table><tbody>
<!-- START TABLE -->
<!-- TABLE HEADER -->
<th valign="bottom"></th>
<th valign="bottom">ViT-Tiny</th>
<th valign="bottom">ViT-Small</th>
<th valign="bottom">ViT-Base</th>
<!-- TABLE BODY -->
<tr><td align="left">TVL-LLaMA</td>
<td align="center"><a href='https://huggingface.co/mlfu7/Touch-Vision-Language-Models/resolve/main/ckpt/tvl_llama/tvl_llama_vittiny.pth?download=true'>download</a></td>
<td align="center"><a href='https://huggingface.co/mlfu7/Touch-Vision-Language-Models/resolve/main/ckpt/tvl_llama/tvl_llama_vits.pth?download=true'>download</a></td>
<td align="center"><a href='https://huggingface.co/mlfu7/Touch-Vision-Language-Models/resolve/main/ckpt/tvl_llama/tvl_llama_vitb.pth?download=true'>download</a></td>
</tr>
<tr><td align="left">Reference TVL Benchmark Score (1-10)</td>
<td align="center">5.03</td>
<td align="center">5.01</td>
<td align="center"> 4.87</td>
</tr>
</tbody></table>
